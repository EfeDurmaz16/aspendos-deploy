Åžimdi detailed document yazacaÄŸÄ±m:

# ASPENDOS: Rate Limiting & Multi-Modal File Indexing Pipeline

## Part 1: Rate Limiting System (Abuse Prevention)

### Problem Statement

Your AI application will face these attack vectors:

```
Abuse Scenario 1: User uploads 100 files Ã— 100MB each = 10GB in minutes
                  â†’ Fills storage quota
                  â†’ Costs you $1,000 in bandwidth
                  â†’ Blocks legitimate users

Abuse Scenario 2: User sends 1 million token requests in 1 hour
                  â†’ $1,000 in API costs
                  â†’ Rate limiting cost: $0

Abuse Scenario 3: Bot tries 10,000 concurrent file processing jobs
                  â†’ Crashes your worker pool
                  â†’ All users experience 500 errors
```

***

### Multi-Layer Rate Limiting Strategy

```python
# services/rate_limiter.py
from redis import Redis
from enum import Enum
from datetime import datetime, timedelta
from typing import Tuple, Optional
import json

class UserTier(Enum):
    """User tier classification with different limits"""
    FREE = {
        "requests_per_minute": 10,
        "requests_per_hour": 100,
        "daily_tokens": 10_000,
        "daily_file_uploads_gb": 0.5,
        "max_file_size_mb": 10,
        "max_concurrent_jobs": 1,
        "embedding_requests_per_day": 100,
        "cache_ttl": 300  # 5 minutes
    }
    
    PRO = {
        "requests_per_minute": 60,
        "requests_per_hour": 1_000,
        "daily_tokens": 500_000,
        "daily_file_uploads_gb": 50,
        "max_file_size_mb": 250,
        "max_concurrent_jobs": 5,
        "embedding_requests_per_day": 5_000,
        "cache_ttl": 3600  # 1 hour
    }
    
    ENTERPRISE = {
        "requests_per_minute": 500,
        "requests_per_hour": 50_000,
        "daily_tokens": 10_000_000,
        "daily_file_uploads_gb": 500,
        "max_file_size_mb": 1000,
        "max_concurrent_jobs": 50,
        "embedding_requests_per_day": 100_000,
        "cache_ttl": 86400  # 24 hours
    }


class RateLimiter:
    """
    Multi-layer rate limiting with Token Bucket algorithm
    """
    
    def __init__(self, dragonfly_url: str):
        self.redis = Redis.from_url(dragonfly_url, decode_responses=True)
        self.algorithms = {
            "token_bucket": self._token_bucket,
            "sliding_window": self._sliding_window,
            "resource_quota": self._resource_quota
        }
    
    async def check_request(
        self,
        user_id: str,
        endpoint: str,
        resource_type: str = "general"
    ) -> Tuple[bool, dict]:
        """
        Check if request should be allowed
        
        Returns:
            (allowed: bool, metadata: dict with retry info)
        """
        
        # Layer 1: Get user tier
        user_tier = await self._get_user_tier(user_id)
        tier_limits = user_tier.value
        
        # Layer 2: Check token bucket (main rate limiter)
        bucket_allowed, bucket_info = await self._token_bucket(
            user_id=user_id,
            limit=tier_limits["requests_per_minute"],
            window_seconds=60
        )
        
        if not bucket_allowed:
            return False, {
                "retry_after": bucket_info["retry_after"],
                "limit": bucket_info["limit"],
                "remaining": bucket_info["remaining"],
                "reset": bucket_info["reset"],
                "reason": "rate_limit_exceeded"
            }
        
        # Layer 3: Check resource-specific quotas
        resource_allowed, resource_info = await self._resource_quota(
            user_id=user_id,
            resource_type=resource_type,
            limits=tier_limits
        )
        
        if not resource_allowed:
            return False, {
                "retry_after": resource_info["retry_after"],
                "quota_used": resource_info["used"],
                "quota_limit": resource_info["limit"],
                "resets_at": resource_info["resets_at"],
                "reason": f"{resource_type}_quota_exceeded"
            }
        
        # Layer 4: Check endpoint-specific limits
        endpoint_allowed, endpoint_info = await self._endpoint_limit(
            user_id=user_id,
            endpoint=endpoint,
            tier_limits=tier_limits
        )
        
        if not endpoint_allowed:
            return False, endpoint_info
        
        return True, {
            "allowed": True,
            "requests_remaining": bucket_info["remaining"],
            "quota_remaining": resource_info["remaining"],
            "reset_in_seconds": bucket_info["reset"]
        }
    
    async def _token_bucket(
        self,
        user_id: str,
        limit: int,
        window_seconds: int
    ) -> Tuple[bool, dict]:
        """
        Token Bucket Algorithm: Smooth out bursts
        
        Visualization:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Bucket: 60 tokens           â”‚
        â”‚  Refill rate: 1 token/sec    â”‚
        â”‚  Current tokens: 45          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
        When request comes:
        - If tokens > 0: Allow, remove 1 token
        - If tokens = 0: Deny, tell when token will be available
        """
        
        key = f"rate_limit:token_bucket:{user_id}"
        
        # Get current bucket state
        bucket = await self.redis.hgetall(key)
        
        if not bucket:
            # First request: fill bucket
            now = datetime.now()
            bucket = {
                "tokens": limit,
                "last_refill": now.timestamp(),
                "capacity": limit
            }
        else:
            bucket = {k: float(v) if k != "capacity" else int(v) for k, v in bucket.items()}
        
        # Refill tokens based on time elapsed
        now = datetime.now()
        time_elapsed = now.timestamp() - bucket["last_refill"]
        refill_rate = bucket["capacity"] / window_seconds
        tokens_to_add = time_elapsed * refill_rate
        
        bucket["tokens"] = min(bucket["capacity"], bucket["tokens"] + tokens_to_add)
        bucket["last_refill"] = now.timestamp()
        
        # Check if we can take a token
        allowed = bucket["tokens"] >= 1
        
        if allowed:
            bucket["tokens"] -= 1
        
        # Save bucket state
        await self.redis.hset(key, mapping={
            "tokens": bucket["tokens"],
            "last_refill": bucket["last_refill"],
            "capacity": bucket["capacity"]
        })
        await self.redis.expire(key, window_seconds * 2)
        
        return allowed, {
            "limit": limit,
            "remaining": int(bucket["tokens"]),
            "reset": window_seconds,
            "retry_after": int((1 - bucket["tokens"]) / refill_rate) if not allowed else 0
        }
    
    async def _sliding_window(
        self,
        user_id: str,
        limit: int,
        window_seconds: int
    ) -> Tuple[bool, dict]:
        """
        Sliding Window: More accurate but slightly more CPU-intensive
        """
        
        key = f"rate_limit:sliding:{user_id}"
        now = datetime.now()
        window_start = (now - timedelta(seconds=window_seconds)).timestamp()
        
        # Remove old entries outside the window
        await self.redis.zremrangebyscore(key, 0, window_start)
        
        # Count requests in current window
        request_count = await self.redis.zcard(key)
        
        allowed = request_count < limit
        
        if allowed:
            # Add current request
            await self.redis.zadd(key, {str(now.timestamp()): now.timestamp()})
            await self.redis.expire(key, window_seconds)
        
        return allowed, {
            "limit": limit,
            "remaining": limit - request_count,
            "reset": window_seconds
        }
    
    async def _resource_quota(
        self,
        user_id: str,
        resource_type: str,
        limits: dict
    ) -> Tuple[bool, dict]:
        """
        Resource-specific quotas (tokens, file uploads, etc)
        
        Tracks per-day quotas:
        - daily_tokens: Total tokens consumed
        - daily_file_uploads_gb: Total GB uploaded
        - max_concurrent_jobs: Current running jobs
        """
        
        now = datetime.now()
        reset_time = (now + timedelta(days=1)).replace(
            hour=0, minute=0, second=0, microsecond=0
        )
        
        quota_key = f"quota:{user_id}:{resource_type}:{now.date()}"
        concurrent_key = f"concurrent:{user_id}:{resource_type}"
        
        quota = await self.redis.hgetall(quota_key)
        current_concurrent = await self.redis.incr(concurrent_key)
        
        # Define quota limits based on resource type
        quota_limits = {
            "tokens": limits["daily_tokens"],
            "file_uploads_gb": limits["daily_file_uploads_gb"],
            "embedding_requests": limits["embedding_requests_per_day"]
        }
        
        if resource_type not in quota_limits:
            return True, {"used": 0, "limit": float("inf"), "remaining": float("inf"), "resets_at": None}
        
        used = int(quota.get("used", 0))
        limit = quota_limits[resource_type]
        
        allowed = used < limit and current_concurrent <= limits["max_concurrent_jobs"]
        
        seconds_until_reset = (reset_time - now).total_seconds()
        
        return allowed, {
            "used": used,
            "limit": limit,
            "remaining": max(0, limit - used),
            "resets_at": reset_time.isoformat(),
            "retry_after": int(seconds_until_reset) if not allowed else 0,
            "concurrent": current_concurrent,
            "max_concurrent": limits["max_concurrent_jobs"]
        }
    
    async def _endpoint_limit(
        self,
        user_id: str,
        endpoint: str,
        tier_limits: dict
    ) -> Tuple[bool, dict]:
        """
        Endpoint-specific limits (some endpoints are more expensive)
        """
        
        # Expensive endpoints get stricter limits
        endpoint_multipliers = {
            "/api/index/upload": 0.1,      # 10% of normal limit
            "/api/query/multimodal": 0.5,  # 50% of normal limit
            "/api/export/pdf": 0.2,        # 20% of normal limit
            "/api/batch-process": 0.05,    # 5% of normal limit
        }
        
        multiplier = endpoint_multipliers.get(endpoint, 1.0)
        adjusted_limit = int(tier_limits["requests_per_hour"] * multiplier)
        
        key = f"endpoint_limit:{user_id}:{endpoint}:{datetime.now().date()}"
        count = await self.redis.incr(key)
        await self.redis.expire(key, 86400)
        
        allowed = count <= adjusted_limit
        
        return allowed, {
            "used": count,
            "limit": adjusted_limit,
            "remaining": max(0, adjusted_limit - count),
            "retry_after": 3600 if not allowed else 0,
            "reason": f"endpoint_{endpoint}_limit"
        }
    
    async def consume_resource(
        self,
        user_id: str,
        resource_type: str,
        amount: float
    ) -> dict:
        """
        Consume quota (called after successful processing)
        """
        
        now = datetime.now()
        quota_key = f"quota:{user_id}:{resource_type}:{now.date()}"
        
        await self.redis.hincrbyfloat(quota_key, "used", amount)
        await self.redis.expire(quota_key, 86400)
        
        quota = await self.redis.hgetall(quota_key)
        
        return {
            "used": float(quota.get("used", 0)),
            "consumed": amount,
            "timestamp": now.isoformat()
        }
    
    async def _get_user_tier(self, user_id: str) -> UserTier:
        """Get user's tier from database"""
        tier_str = await self.redis.get(f"user:tier:{user_id}")
        return UserTier[tier_str] if tier_str else UserTier.FREE


# API Usage in FastAPI
from fastapi import FastAPI, UploadFile, HTTPException
from fastapi.responses import JSONResponse

app = FastAPI()
rate_limiter = RateLimiter("redis://dragonfly:6379")

@app.post("/api/index/upload")
async def upload_file(
    file: UploadFile,
    user_id: str,
    request: Request
):
    """Upload and index file"""
    
    # Layer 1: Check if request is allowed
    allowed, metadata = await rate_limiter.check_request(
        user_id=user_id,
        endpoint="/api/index/upload",
        resource_type="file_uploads_gb"
    )
    
    if not allowed:
        return JSONResponse(
            status_code=429,
            content={
                "error": "Too Many Requests",
                "message": metadata.get("reason"),
                "retry_after": metadata.get("retry_after"),
                "quota_used": metadata.get("quota_used"),
                "quota_limit": metadata.get("quota_limit")
            },
            headers={"Retry-After": str(metadata.get("retry_after", 60))}
        )
    
    # Layer 2: Check file size
    file_size_mb = len(await file.read()) / (1024 * 1024)
    file_size_gb = file_size_mb / 1024
    await file.seek(0)  # Reset file pointer
    
    user_tier = await rate_limiter._get_user_tier(user_id)
    max_size = user_tier.value["max_file_size_mb"]
    
    if file_size_mb > max_size:
        return JSONResponse(
            status_code=413,
            content={
                "error": "Payload Too Large",
                "max_file_size_mb": max_size,
                "received_file_size_mb": file_size_mb
            }
        )
    
    try:
        # Process file (actual indexing)
        result = await process_multimodal_file(file, user_id)
        
        # Layer 3: Consume quota
        consumption = await rate_limiter.consume_resource(
            user_id=user_id,
            resource_type="file_uploads_gb",
            amount=file_size_gb
        )
        
        return {
            "success": True,
            "file_id": result["file_id"],
            "file_size_gb": file_size_gb,
            "tokens_consumed": result.get("tokens", 0),
            "quota_remaining_gb": metadata["quota_remaining"],
            "requests_remaining": metadata["requests_remaining"]
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )
    
    finally:
        # Always decrement concurrent job counter
        await rate_limiter.redis.decr(f"concurrent:{user_id}:file_uploads_gb")


@app.get("/api/rate-limit/status")
async def get_rate_limit_status(user_id: str):
    """Show user their current rate limit status"""
    
    user_tier = await rate_limiter._get_user_tier(user_id)
    limits = user_tier.value
    
    now = datetime.now()
    
    # Get current quotas
    tokens_quota = await rate_limiter.redis.hgetall(
        f"quota:{user_id}:tokens:{now.date()}"
    )
    upload_quota = await rate_limiter.redis.hgetall(
        f"quota:{user_id}:file_uploads_gb:{now.date()}"
    )
    
    return {
        "tier": user_tier.name,
        "limits": limits,
        "current_usage": {
            "tokens": {
                "used": int(tokens_quota.get("used", 0)),
                "limit": limits["daily_tokens"],
                "remaining": limits["daily_tokens"] - int(tokens_quota.get("used", 0))
            },
            "file_uploads_gb": {
                "used": float(upload_quota.get("used", 0)),
                "limit": limits["daily_file_uploads_gb"],
                "remaining": limits["daily_file_uploads_gb"] - float(upload_quota.get("used", 0))
            }
        },
        "resets_at": (now + timedelta(days=1)).replace(
            hour=0, minute=0, second=0, microsecond=0
        ).isoformat()
    }
```

***

## Part 2: Multi-Modal File Indexing Pipeline

### Supported File Types & Conversion Strategy

```python
# services/file_processor.py
from markitdown import MarkItDown
from pathlib import Path
import tempfile
import asyncio
from typing import Dict, List, Optional
import json
import logging

logger = logging.getLogger(__name__)

class MultiModalFileProcessor:
    """
    Convert any file type to structured format for LlamaIndex indexing
    """
    
    def __init__(self):
        self.markitdown = MarkItDown()
        
        # File type configurations
        self.processors = {
            # Office Documents
            ".pdf": self._process_pdf,
            ".docx": self._process_docx,
            ".doc": self._process_docx,
            ".xlsx": self._process_xlsx,
            ".xls": self._process_xlsx,
            ".pptx": self._process_pptx,
            ".ppt": self._process_pptx,
            ".pages": self._process_pages,
            
            # Images
            ".jpg": self._process_image,
            ".jpeg": self._process_image,
            ".png": self._process_image,
            ".gif": self._process_image,
            ".bmp": self._process_image,
            ".webp": self._process_image,
            
            # Audio
            ".mp3": self._process_audio,
            ".wav": self._process_audio,
            ".flac": self._process_audio,
            ".m4a": self._process_audio,
            
            # Video (extract frames + transcribe)
            ".mp4": self._process_video,
            ".mov": self._process_video,
            ".mkv": self._process_video,
            ".avi": self._process_video,
            
            # Archives
            ".zip": self._process_zip,
            ".rar": self._process_zip,
            ".7z": self._process_zip,
            ".tar": self._process_zip,
            ".gz": self._process_zip,
            
            # Data Files
            ".csv": self._process_csv,
            ".json": self._process_json,
            ".xml": self._process_xml,
            ".html": self._process_html,
            ".txt": self._process_text,
            
            # Code Files
            ".py": self._process_code,
            ".js": self._process_code,
            ".ts": self._process_code,
            ".java": self._process_code,
            ".cpp": self._process_code,
            ".go": self._process_code,
            ".rs": self._process_code,
        }
    
    async def process_file(
        self,
        file_path: str,
        user_id: str,
        metadata: Optional[Dict] = None
    ) -> Dict:
        """
        Main entry point: Process any file type
        
        Returns:
            {
                "file_id": str,
                "original_name": str,
                "file_type": str,
                "content_markdown": str,
                "structured_data": dict (for tables, code, etc),
                "metadata": dict,
                "tokens_estimate": int,
                "processing_time_ms": float,
                "extraction_confidence": float
            }
        """
        
        import time
        start_time = time.time()
        
        path = Path(file_path)
        file_ext = path.suffix.lower()
        
        # Get processor for this file type
        processor = self.processors.get(file_ext, self._process_generic)
        
        # Process the file
        result = await processor(file_path)
        
        # Add metadata
        result.update({
            "file_id": self._generate_file_id(user_id, path.name),
            "original_name": path.name,
            "file_type": file_ext,
            "user_id": user_id,
            "metadata": metadata or {},
            "processing_time_ms": (time.time() - start_time) * 1000,
            "tokens_estimate": self._estimate_tokens(result.get("content_markdown", ""))
        })
        
        return result
    
    async def _process_pdf(self, file_path: str) -> Dict:
        """
        Process PDF with MarkItDown
        MarkItDown uses:
        - pdfminer for layout analysis
        - PyMuPDF for text extraction
        """
        
        result = await asyncio.to_thread(
            self.markitdown.convert,
            file_path
        )
        
        return {
            "content_markdown": result.text_content,
            "structured_data": {
                "type": "document",
                "pages": self._count_pages(result),
                "tables": self._extract_table_info(result),
                "images": self._extract_image_info(result)
            },
            "extraction_confidence": 0.95
        }
    
    async def _process_docx(self, file_path: str) -> Dict:
        """
        Process Word documents with MarkItDown
        Uses mammoth library for DOCX conversion
        """
        
        result = await asyncio.to_thread(
            self.markitdown.convert,
            file_path
        )
        
        return {
            "content_markdown": result.text_content,
            "structured_data": {
                "type": "document",
                "formatted_text": True,
                "tables": self._extract_table_info(result),
                "images": self._extract_image_info(result)
            },
            "extraction_confidence": 0.98
        }
    
    async def _process_xlsx(self, file_path: str) -> Dict:
        """
        Process Excel files with MarkItDown
        Uses pandas + openpyxl for XLSX conversion
        """
        
        result = await asyncio.to_thread(
            self.markitdown.convert,
            file_path
        )
        
        # Also extract structured table data
        import pandas as pd
        excel_data = {}
        try:
            excel_file = pd.ExcelFile(file_path)
            for sheet in excel_file.sheet_names:
                excel_data[sheet] = pd.read_excel(
                    file_path,
                    sheet_name=sheet
                ).to_dict(orient="records")
        except Exception as e:
            logger.warning(f"Could not extract structured Excel data: {e}")
        
        return {
            "content_markdown": result.text_content,
            "structured_data": {
                "type": "spreadsheet",
                "sheets": excel_data,
                "table_count": len(excel_data)
            },
            "extraction_confidence": 0.99
        }
    
    async def _process_pptx(self, file_path: str) -> Dict:
        """
        Process PowerPoint files with MarkItDown
        Uses python-pptx for PPTX conversion
        """
        
        result = await asyncio.to_thread(
            self.markitdown.convert,
            file_path
        )
        
        # Also extract slide structure
        from pptx import Presentation
        slides_info = []
        try:
            prs = Presentation(file_path)
            for i, slide in enumerate(prs.slides):
                slide_text = []
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        slide_text.append(shape.text)
                
                slides_info.append({
                    "slide_number": i + 1,
                    "content": " ".join(slide_text),
                    "has_images": any(shape.shape_type == 13 for shape in slide.shapes)
                })
        except Exception as e:
            logger.warning(f"Could not extract slide structure: {e}")
        
        return {
            "content_markdown": result.text_content,
            "structured_data": {
                "type": "presentation",
                "slides": slides_info,
                "slide_count": len(slides_info)
            },
            "extraction_confidence": 0.92
        }
    
    async def _process_pages(self, file_path: str) -> Dict:
        """
        Process Apple Pages documents
        Requires pandoc for conversion
        """
        
        import subprocess
        import tempfile
        
        # Convert Pages to DOCX first, then process
        with tempfile.NamedTemporaryFile(suffix=".docx", delete=False) as tmp:
            try:
                subprocess.run(
                    ["pandoc", file_path, "-o", tmp.name],
                    check=True,
                    capture_output=True
                )
                return await self._process_docx(tmp.name)
            except Exception as e:
                logger.error(f"Pages processing failed: {e}")
                return {
                    "content_markdown": "",
                    "structured_data": {},
                    "extraction_confidence": 0.0,
                    "error": str(e)
                }
    
    async def _process_image(self, file_path: str) -> Dict:
        """
        Process images: OCR + Vision understanding
        """
        
        # 1. Extract text with OCR
        from PIL import Image
        import pytesseract
        
        ocr_text = ""
        try:
            img = Image.open(file_path)
            ocr_text = pytesseract.image_to_string(img)
        except Exception as e:
            logger.warning(f"OCR failed: {e}")
        
        # 2. Extract EXIF metadata
        exif_data = {}
        try:
            from PIL.ExifTags import TAGS
            img = Image.open(file_path)
            exif = img._getexif()
            if exif:
                exif_data = {
                    TAGS.get(k, k): v for k, v in exif.items()
                }
        except Exception as e:
            logger.warning(f"EXIF extraction failed: {e}")
        
        # 3. Vision API analysis (Claude Vision)
        vision_analysis = await self._analyze_image_with_vision(file_path)
        
        return {
            "content_markdown": f"# Image Analysis\n\n{vision_analysis}\n\n## Extracted Text\n\n{ocr_text}",
            "structured_data": {
                "type": "image",
                "ocr_text": ocr_text,
                "vision_analysis": vision_analysis,
                "exif_metadata": exif_data,
                "file_size": Path(file_path).stat().st_size
            },
            "extraction_confidence": 0.85
        }
    
    async def _process_audio(self, file_path: str) -> Dict:
        """
        Process audio: Transcription + metadata
        """
        
        # 1. Transcribe with Whisper
        transcription = await self._transcribe_audio(file_path)
        
        # 2. Extract metadata
        metadata = self._extract_audio_metadata(file_path)
        
        return {
            "content_markdown": f"# Audio Transcription\n\n{transcription}",
            "structured_data": {
                "type": "audio",
                "transcription": transcription,
                "metadata": metadata,
                "duration_seconds": metadata.get("duration_seconds", 0)
            },
            "extraction_confidence": 0.90
        }
    
    async def _process_video(self, file_path: str) -> Dict:
        """
        Process video: Extract frames + transcribe audio
        """
        
        import cv2
        import tempfile
        
        frames_analysis = []
        transcription = ""
        metadata = {}
        
        try:
            # 1. Extract key frames
            cap = cv2.VideoCapture(file_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps if fps > 0 else 0
            
            # Extract every Nth frame (e.g., every 30 frames)
            frame_interval = max(1, int(fps * 5))  # Every 5 seconds
            frame_count = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if frame_count % frame_interval == 0:
                    # Save frame temporarily
                    with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as tmp:
                        cv2.imwrite(tmp.name, frame)
                        
                        # Analyze frame with vision
                        frame_analysis = await self._analyze_image_with_vision(tmp.name)
                        frames_analysis.append({
                            "timestamp": frame_count / fps,
                            "analysis": frame_analysis
                        })
                
                frame_count += 1
            
            cap.release()
            
            metadata = {
                "duration_seconds": duration,
                "fps": fps,
                "total_frames": total_frames,
                "key_frames_extracted": len(frames_analysis)
            }
            
            # 2. Extract audio and transcribe
            transcription = await self._extract_and_transcribe_video_audio(file_path)
            
        except Exception as e:
            logger.error(f"Video processing failed: {e}")
        
        return {
            "content_markdown": f"""# Video Analysis

## Transcription
{transcription}

## Key Frames
{json.dumps([f['analysis'] for f in frames_analysis[:5]], indent=2)}
""",
            "structured_data": {
                "type": "video",
                "metadata": metadata,
                "key_frames": frames_analysis,
                "audio_transcription": transcription
            },
            "extraction_confidence": 0.80
        }
    
    async def _process_zip(self, file_path: str) -> Dict:
        """
        Process ZIP/Archive files: Extract and process contents
        """
        
        import zipfile
        import tempfile
        
        contents = []
        with tempfile.TemporaryDirectory() as tmpdir:
            try:
                with zipfile.ZipFile(file_path, 'r') as zip_ref:
                    zip_ref.extractall(tmpdir)
                
                # Process each file in archive
                for root, dirs, files in os.walk(tmpdir):
                    for file in files:
                        file_path_in_zip = os.path.join(root, file)
                        file_ext = Path(file).suffix.lower()
                        
                        # Skip unsupported files
                        if file_ext in self.processors:
                            try:
                                result = await self.process_file(
                                    file_path_in_zip,
                                    user_id="archive_processor"
                                )
                                contents.append(result)
                            except Exception as e:
                                logger.warning(f"Failed to process {file}: {e}")
            
            except Exception as e:
                logger.error(f"ZIP processing failed: {e}")
        
        # Combine all content
        combined_markdown = "\n\n---\n\n".join([
            f"## {c['original_name']}\n\n{c['content_markdown']}"
            for c in contents
        ])
        
        return {
            "content_markdown": combined_markdown,
            "structured_data": {
                "type": "archive",
                "files_processed": len(contents),
                "contents": [
                    {
                        "name": c["original_name"],
                        "type": c["file_type"],
                        "tokens": c.get("tokens_estimate", 0)
                    }
                    for c in contents
                ]
            },
            "extraction_confidence": 0.75
        }
    
    async def _process_csv(self, file_path: str) -> Dict:
        """Process CSV files"""
        
        import pandas as pd
        
        try:
            df = pd.read_csv(file_path)
            
            markdown = df.to_markdown()
            
            return {
                "content_markdown": markdown,
                "structured_data": {
                    "type": "csv",
                    "rows": len(df),
                    "columns": len(df.columns),
                    "column_names": list(df.columns),
                    "sample_rows": df.head(5).to_dict(orient="records")
                },
                "extraction_confidence": 0.99
            }
        except Exception as e:
            logger.error(f"CSV processing failed: {e}")
            return {"content_markdown": "", "structured_data": {}, "error": str(e)}
    
    async def _process_json(self, file_path: str) -> Dict:
        """Process JSON files"""
        
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            markdown = f"```json\n{json.dumps(data, indent=2)}\n```"
            
            return {
                "content_markdown": markdown,
                "structured_data": {
                    "type": "json",
                    "data": data,
                    "keys": list(data.keys()) if isinstance(data, dict) else None
                },
                "extraction_confidence": 1.0
            }
        except Exception as e:
            logger.error(f"JSON processing failed: {e}")
            return {"content_markdown": "", "structured_data": {}, "error": str(e)}
    
    async def _process_xml(self, file_path: str) -> Dict:
        """Process XML files"""
        
        from xml.etree import ElementTree as ET
        
        try:
            tree = ET.parse(file_path)
            root = tree.getroot()
            
            # Convert to markdown
            markdown = self._xml_to_markdown(root)
            
            return {
                "content_markdown": markdown,
                "structured_data": {
                    "type": "xml",
                    "root_tag": root.tag,
                    "elements_count": len(list(root.iter()))
                },
                "extraction_confidence": 0.95
            }
        except Exception as e:
            logger.error(f"XML processing failed: {e}")
            return {"content_markdown": "", "structured_data": {}, "error": str(e)}
    
    async def _process_html(self, file_path: str) -> Dict:
        """Process HTML files with MarkItDown"""
        
        result = await asyncio.to_thread(
            self.markitdown.convert,
            file_path
        )
        
        return {
            "content_markdown": result.text_content,
            "structured_data": {
                "type": "html",
                "links": self._extract_links(result)
            },
            "extraction_confidence": 0.90
        }
    
    async def _process_text(self, file_path: str) -> Dict:
        """Process plain text files"""
        
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        return {
            "content_markdown": content,
            "structured_data": {
                "type": "text",
                "lines": len(content.split('\n')),
                "characters": len(content)
            },
            "extraction_confidence": 1.0
        }
    
    async def _process_code(self, file_path: str) -> Dict:
        """Process code files"""
        
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Detect language from extension
        ext_to_lang = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".java": "java",
            ".cpp": "cpp",
            ".go": "go",
            ".rs": "rust"
        }
        
        lang = ext_to_lang.get(Path(file_path).suffix.lower(), "")
        
        return {
            "content_markdown": f"```{lang}\n{content}\n```",
            "structured_data": {
                "type": "code",
                "language": lang,
                "lines": len(content.split('\n')),
                "functions": self._extract_functions(content, lang)
            },
            "extraction_confidence": 0.98
        }
    
    async def _process_generic(self, file_path: str) -> Dict:
        """Fallback for unsupported file types"""
        
        try:
            result = await asyncio.to_thread(
                self.markitdown.convert,
                file_path
            )
            
            return {
                "content_markdown": result.text_content,
                "structured_data": {
                    "type": "generic",
                    "processor": "markitdown_fallback"
                },
                "extraction_confidence": 0.60
            }
        except Exception as e:
            logger.error(f"Generic processing failed: {e}")
            return {"content_markdown": "", "structured_data": {}, "error": str(e)}
    
    # Helper methods
    
    async def _analyze_image_with_vision(self, image_path: str) -> str:
        """Use Claude Vision API to analyze image"""
        
        from anthropic import Anthropic
        from base64 import b64encode
        
        try:
            client = Anthropic()
            
            with open(image_path, "rb") as f:
                image_data = b64encode(f.read()).decode("utf-8")
            
            message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1024,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/jpeg",
                                    "data": image_data
                                }
                            },
                            {
                                "type": "text",
                                "text": "Analyze this image comprehensively. Describe objects, text, people, layout, context, and any notable elements."
                            }
                        ]
                    }
                ]
            )
            
            return message.content[0].text
        
        except Exception as e:
            logger.warning(f"Vision analysis failed: {e}")
            return ""
    
    async def _transcribe_audio(self, audio_path: str) -> str:
        """Transcribe audio with Whisper"""
        
        from openai import OpenAI
        
        try:
            client = OpenAI()
            
            with open(audio_path, "rb") as f:
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=f
                )
            
            return transcript.text
        
        except Exception as e:
            logger.warning(f"Audio transcription failed: {e}")
            return ""
    
    async def _extract_and_transcribe_video_audio(self, video_path: str) -> str:
        """Extract audio from video and transcribe"""
        
        import subprocess
        import tempfile
        
        try:
            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmp:
                # Extract audio with ffmpeg
                subprocess.run(
                    [
                        "ffmpeg",
                        "-i", video_path,
                        "-q:a", "9",
                        "-n",
                        tmp.name
                    ],
                    check=True,
                    capture_output=True
                )
                
                # Transcribe
                return await self._transcribe_audio(tmp.name)
        
        except Exception as e:
            logger.warning(f"Video audio extraction failed: {e}")
            return ""
    
    def _estimate_tokens(self, text: str) -> int:
        """Rough token estimate (1 token â‰ˆ 4 characters)"""
        return len(text) // 4
    
    def _generate_file_id(self, user_id: str, filename: str) -> str:
        """Generate unique file ID"""
        import hashlib
        from datetime import datetime
        
        hash_input = f"{user_id}:{filename}:{datetime.now().isoformat()}"
        return hashlib.sha256(hash_input.encode()).hexdigest()[:16]
    
    def _count_pages(self, result) -> int:
        # Implementation
        return 0
    
    def _extract_table_info(self, result) -> list:
        # Implementation
        return []
    
    def _extract_image_info(self, result) -> list:
        # Implementation
        return []
    
    def _extract_audio_metadata(self, file_path: str) -> dict:
        # Implementation
        return {}
    
    def _extract_links(self, result) -> list:
        # Implementation
        return []
    
    def _xml_to_markdown(self, element) -> str:
        # Implementation
        return ""
    
    def _extract_functions(self, code: str, language: str) -> list:
        # Implementation
        return []
```

***

## Part 3: LlamaIndex Integration + Export Pipeline

```python
# services/indexing_service.py
from llama_index.core import VectorStoreIndex
from llama_index.vector_stores.redis import RedisVectorStore
from llama_index.storage.docstore.redis import RedisDocumentStore
from llama_index.storage.index_store.redis import RedisIndexStore
from llama_index.core import StorageContext
from llama_index.core.node_parser import SentenceSplitter
from redis import Redis

class IndexingService:
    """
    Index multimodal content in LlamaIndex
    """
    
    def __init__(self, dragonfly_url: str):
        self.redis = Redis.from_url(dragonfly_url, decode_responses=True)
        
        # Initialize storage layers
        self.docstore = RedisDocumentStore.from_host_and_port(
            host="dragonfly",
            port=6379,
            namespace="aspendos"
        )
        
        self.vector_store = RedisVectorStore(
            redis_client=self.redis,
            index_name="aspendos_vectors"
        )
        
        self.index_store = RedisIndexStore.from_host_and_port(
            host="dragonfly",
            port=6379,
            namespace="aspendos"
        )
        
        self.storage_context = StorageContext.from_defaults(
            docstore=self.docstore,
            vector_store=self.vector_store,
            index_store=self.index_store
        )
    
    async def index_multimodal_content(
        self,
        file_id: str,
        content: str,
        structured_data: dict,
        metadata: dict
    ) -> dict:
        """Index processed file content"""
        
        # Split into nodes
        splitter = SentenceSplitter(
            chunk_size=512,
            chunk_overlap=50
        )
        
        nodes = splitter.get_nodes_from_documents(
            [Document(text=content, metadata=metadata)]
        )
        
        # Create index
        index = VectorStoreIndex(
            nodes,
            storage_context=self.storage_context
        )
        
        return {
            "file_id": file_id,
            "nodes_created": len(nodes),
            "index_id": index.index_id,
            "indexed_at": datetime.now().isoformat()
        }
```

***

## Part 4: Export to Multiple Formats

```python
# services/export_service.py
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from docx import Document
import openpyxl
from pptx import Presentation

class ExportService:
    """
    Export indexed content to PDF, Word, Excel, etc.
    """
    
    async def export_to_pdf(
        self,
        file_id: str,
        content_markdown: str,
        structured_data: dict
    ) -> bytes:
        """Export to PDF"""
        
        from markdown2 import markdown
        from weasyprint import HTML, CSS
        from io import BytesIO
        
        html_content = markdown(content_markdown)
        
        doc = HTML(string=html_content)
        pdf_bytes = doc.write_pdf()
        
        return pdf_bytes
    
    async def export_to_docx(
        self,
        file_id: str,
        content_markdown: str,
        structured_data: dict
    ) -> bytes:
        """Export to Word document"""
        
        from markdown2 import markdown
        from io import BytesIO
        from pandoc import pandoc_convert
        
        doc = Document()
        doc.add_heading(f"Export: {file_id}", level=1)
        
        # Parse markdown and add to document
        lines = content_markdown.split('\n')
        for line in lines:
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                doc.add_heading(line.lstrip('#').strip(), level=level)
            elif line.strip():
                doc.add_paragraph(line)
        
        # Save to bytes
        output = BytesIO()
        doc.save(output)
        return output.getvalue()
    
    async def export_to_xlsx(
        self,
        file_id: str,
        content_markdown: str,
        structured_data: dict
    ) -> bytes:
        """Export to Excel"""
        
        from io import BytesIO
        import pandas as pd
        
        wb = openpyxl.Workbook()
        ws = wb.active
        
        # Add metadata
        ws['A1'] = f"File: {file_id}"
        ws['A2'] = f"Exported: {datetime.now().isoformat()}"
        
        # Add content
        row = 4
        for line in content_markdown.split('\n'):
            if line.strip():
                ws[f'A{row}'] = line
                row += 1
        
        output = BytesIO()
        wb.save(output)
        return output.getvalue()
```

***

## Summary: Protection + Processing

```yaml
RATE LIMITING PROTECTION:
â”œâ”€ Layer 1: Token Bucket (60 req/min for free tier)
â”œâ”€ Layer 2: Resource Quotas (500MB/day files)
â”œâ”€ Layer 3: Endpoint-Specific Limits (uploads 10% of normal)
â””â”€ Layer 4: Concurrent Job Limits (max 1 job free tier)

MULTIMODAL PROCESSING:
â”œâ”€ Input: 17+ file types
â”œâ”€ Conversion: MarkItDown + specialized processors
â”œâ”€ Processing: OCR, transcription, table extraction
â”œâ”€ Storage: LlamaIndex (Document + Vector + Index stores)
â””â”€ Output: PDF, Word, Excel, Markdown exports

PROTECTION AGAINST:
âœ“ 100 files upload â†’ Blocked (quota exceeded)
âœ“ 1M tokens/hour â†’ Blocked (rate limited)
âœ“ Concurrent job spam â†’ Blocked (5 job limit)
âœ“ Invalid formats â†’ Handled gracefully (fallback processor)

COST OPTIMIZATION:
âœ“ Cache hit reduces tokens by 50%
âœ“ Dragonfly handles 25x more concurrent users than Redis
âœ“ Smaller file sizes = lower storage cost
```

Bunu ÅŸimdi **document olarak canvas'a** yazayÄ±m mÄ±, yoksa **implementation guide** olarak devam etmemi mi istersin? ðŸš€